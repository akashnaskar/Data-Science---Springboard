{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, json, random, pickle\n",
    "random.seed(42)\n",
    "# ignore deprecation warnings in sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Specify data directory\n",
    "\n",
    "data_dir = os.path.join(os.path.dirname(os.getcwd()),'Data')\n",
    "\n",
    "# Set model directory\n",
    "\n",
    "model_dir = os.path.join(os.path.dirname(os.getcwd()), 'Model')\n",
    "\n",
    "# Set data paths\n",
    "\n",
    "train_path = os.path.join(data_dir, 'train.csv')\n",
    "\n",
    "train_processed_path = os.path.join(data_dir, 'interim', 'train_preprocessed.txt')\n",
    "\n",
    "meta_feat_path = os.path.join(data_dir, 'interim', 'meta_feat.txt')\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "train_processed = pd.read_json(train_processed_path)\n",
    "meta_feat = pd.read_json(meta_feat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some support function to adjust class weights and \n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "class_weights = compute_sample_weight('balanced', \n",
    "                                      np.unique(train_processed.sentiment),\n",
    "                                      train_processed.sentiment)\n",
    "\n",
    "def get_label(row):\n",
    "    \"\"\"\n",
    "    Get regular label from one hot encoded labels\n",
    "    \"\"\"\n",
    "    for label in [0,1,2]:\n",
    "        if row[label] == 1:\n",
    "            return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Directional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyper-parameters\n",
    "\n",
    "vocab_size = 5000\n",
    "input_length = 120\n",
    "embed_dim = 100\n",
    "lstm_out = 100\n",
    "batch_size = 32\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and build model input\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, split=' ')\n",
    "tokenizer.fit_on_texts(train_processed['text'].values)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(train_processed['text'].values)\n",
    "X = pad_sequences(X, maxlen = input_length)\n",
    "\n",
    "#y = to_categorical(train_processed['sentiment'].values)\n",
    "y = train_processed['sentiment'].values\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, random_state = 42)\n",
    "print(Xtrain.shape,ytrain.shape)\n",
    "print(Xtest.shape,ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define 3-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 42)\n",
    "cvscores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "while 1 == 0:\n",
    "    for train, valid in kfold.split(Xtrain, ytrain):    \n",
    "        # One hot encode label\n",
    "        training_label = to_categorical(ytrain[train])\n",
    "        validation_label = to_categorical(ytrain[valid])\n",
    "\n",
    "        # Build Neural Network architecture\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, \n",
    "                            embed_dim, \n",
    "                            input_length = X.shape[1], \n",
    "                            dropout=0.2))\n",
    "\n",
    "        model.add(Bidirectional(LSTM(lstm_out, \n",
    "                               dropout_U=0.2,\n",
    "                               dropout_W=0.2)))\n",
    "        model.add(Dense(3,\n",
    "                        activation='softmax'))\n",
    "        model.compile(loss = 'categorical_crossentropy', \n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Fit model\n",
    "        model.fit(Xtrain[train], training_label, \n",
    "                  batch_size=batch_size,\n",
    "                  epochs=num_epochs,\n",
    "                  class_weight = class_weights,\n",
    "                  verbose = 0,\n",
    "                  callbacks = [TQDMNotebookCallback()])\n",
    "\n",
    "        # Get model results\n",
    "        scores = model.evaluate(Xtrain[valid], validation_label, verbose = 0)\n",
    "        ypred = model.predict(Xtrain[valid])\n",
    "        ypred_df = pd.DataFrame(ypred)\n",
    "        ypred_max = ypred_df.apply(max, axis = 1)\n",
    "        for index, row in ypred_df.iterrows():\n",
    "            for label, item in row.items():\n",
    "                if item == ypred_max[index]:\n",
    "                    row[label] = 1\n",
    "                else:\n",
    "                    row[label] = 0\n",
    "\n",
    "        # Get confusion matrix\n",
    "        ypred_label = ypred_df.apply(get_label, axis = 1)\n",
    "        yvalid_label = ytrain[valid]\n",
    "\n",
    "        cm = confusion_matrix(yvalid_label, ypred_label)\n",
    "        print(cm, \"\\n\\n\")\n",
    "\n",
    "        f1 = f1_score( yvalid_label, ypred_label, average = 'macro')\n",
    "\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "        print(\"f1: %.2f\" % f1)\n",
    "        cvscores.append(f1)\n",
    "    print(\"%.2f (+- %.2f)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_json = model.to_json()\n",
    "with open(os.path.join(model_dir, \"LSTM_120inputlen_32bsize_5epoch.json\"), 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"LSTM_120inputlen_32bsize_5epoch.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Directional LSTM with Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper-parameters\n",
    "\n",
    "vocab_size = 5000\n",
    "input_length = 120\n",
    "embed_dim = 100\n",
    "lstm_out = 100\n",
    "batch_size = 32\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and build model input\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, split=' ')\n",
    "tokenizer.fit_on_texts(train_processed['text'].values)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(train_processed['text'].values)\n",
    "X = pad_sequences(X, maxlen = input_length)\n",
    "\n",
    "#y = to_categorical(train_processed['sentiment'].values)\n",
    "y = train_processed['sentiment'].values\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, random_state = 42)\n",
    "print(Xtrain.shape,ytrain.shape)\n",
    "print(Xtest.shape,ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 3-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 42)\n",
    "cvscores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "while 1 == 0:\n",
    "    for train, valid in kfold.split(Xtrain, ytrain):    \n",
    "        # One hot encode label\n",
    "        training_label = to_categorical(ytrain[train])\n",
    "        validation_label = to_categorical(ytrain[valid])\n",
    "\n",
    "        # Build Neural Network architecture\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, \n",
    "                            embed_dim, \n",
    "                            input_length = X.shape[1], \n",
    "                            dropout=0.2))\n",
    "\n",
    "        model.add(Bidirectional(LSTM(lstm_out, \n",
    "                               dropout_U=0.2,\n",
    "                               dropout_W=0.2)))\n",
    "        model.add(Dense(3,\n",
    "                        activation='softmax'))\n",
    "        model.compile(loss = 'categorical_crossentropy', \n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Fit model\n",
    "        model.fit(Xtrain[train], training_label, \n",
    "                  batch_size=batch_size,\n",
    "                  epochs=num_epochs,\n",
    "                  class_weight = class_weights,\n",
    "                  verbose = 0,\n",
    "                  callbacks = [TQDMNotebookCallback()])\n",
    "\n",
    "        # Get model results\n",
    "        scores = model.evaluate(Xtrain[valid], validation_label, verbose = 0)\n",
    "        ypred = model.predict(Xtrain[valid])\n",
    "        ypred_df = pd.DataFrame(ypred)\n",
    "        ypred_max = ypred_df.apply(max, axis = 1)\n",
    "        for index, row in ypred_df.iterrows():\n",
    "            for label, item in row.items():\n",
    "                if item == ypred_max[index]:\n",
    "                    row[label] = 1\n",
    "                else:\n",
    "                    row[label] = 0\n",
    "\n",
    "        # Get confusion matrix\n",
    "        ypred_label = ypred_df.apply(get_label, axis = 1)\n",
    "        yvalid_label = ytrain[valid]\n",
    "\n",
    "        cm = confusion_matrix(yvalid_label, ypred_label)\n",
    "        print(cm, \"\\n\\n\")\n",
    "\n",
    "        f1 = f1_score( yvalid_label, ypred_label, average = 'macro')\n",
    "\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "        print(\"f1: %.2f\" % f1)\n",
    "        cvscores.append(f1)\n",
    "    print(\"%.2f (+- %.2f)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(os.path.join(model_dir, \"LSTM_120inputlen_32bsize_5epoch.json\"), 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"LSTM_120inputlen_32bsize_5epoch.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
