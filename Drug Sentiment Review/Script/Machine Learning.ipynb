{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore deprecation warnings in sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data directory\n",
    "\n",
    "data_dir = os.path.join(os.path.dirname(os.getcwd()),'Data')\n",
    "\n",
    "# Set model directory\n",
    "\n",
    "model_dir = os.path.join(os.path.dirname(os.getcwd()), 'Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data paths\n",
    "\n",
    "train_path = os.path.join(data_dir, 'train.csv')\n",
    "\n",
    "train_processed_path = os.path.join(data_dir, 'interim', 'train_preprocessed.txt')\n",
    "\n",
    "meta_feat_path = os.path.join(data_dir, 'interim', 'meta_feat.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path)\n",
    "train_processed = pd.read_json(train_processed_path)\n",
    "meta_feat = pd.read_json(meta_feat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>n_upper</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_wlen</th>\n",
       "      <th>adj_drug_count</th>\n",
       "      <th>n_stop</th>\n",
       "      <th>n_num</th>\n",
       "      <th>drug_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>404</td>\n",
       "      <td>5.315789</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>206</td>\n",
       "      <td>1184</td>\n",
       "      <td>5.747573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>328</td>\n",
       "      <td>1807</td>\n",
       "      <td>5.509146</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>133</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>146</td>\n",
       "      <td>703</td>\n",
       "      <td>4.815068</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>62</td>\n",
       "      <td>6</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>386</td>\n",
       "      <td>5.761194</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment  n_upper  word_count  char_count  avg_wlen  adj_drug_count  \\\n",
       "0             2        1          76         404  5.315789        0.026316   \n",
       "1             2        5         206        1184  5.747573        0.000000   \n",
       "10            1       14         328        1807  5.509146        0.006098   \n",
       "100           1        9         146         703  4.815068        0.006849   \n",
       "1000          2        4          67         386  5.761194        0.029851   \n",
       "\n",
       "      n_stop  n_num  drug_category  \n",
       "0         28      0             38  \n",
       "1         85      0             38  \n",
       "10       133      4             55  \n",
       "100       62      6             89  \n",
       "1000      31      0             35  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Naive Bayes\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from custom_function.ml import create_classifier, evaluate_classifier, get_model_results\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define X and y\n",
    "\n",
    "X = meta_feat.drop(\"sentiment\", axis = 1)\n",
    "y = meta_feat.sentiment\n",
    "indices = train_processed.index\n",
    "\n",
    "# Split train and test set\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'baseline_lr.sav'\n",
    "\n",
    "parameter = {'model__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = StandardScaler())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unique, counts = np.unique(clf.predict(Xtest), return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of word models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "\n",
    "X = train_processed.text\n",
    "y = train_processed.sentiment\n",
    "indices = train_processed.index\n",
    "\n",
    "# Split train and test set\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes - BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'nb_bow.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "             'model__alpha':[0.01, 0.1, 1, 10]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        MultinomialNB(),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = CountVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes - Tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'nb_tfidf.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "             'model__alpha':[0.01, 0.1, 1, 10]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        MultinomialNB(),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = TfidfVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'rf_bow.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "              'pre__ngram_range':[(1,1),(1,2)],\n",
    "              'model__n_estimators':[10 , 20, 40],\n",
    "              'model__max_features': ['auto', 'sqrt', 'log2']}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        RandomForestClassifier(),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = CountVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'rf_tfidf.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "              'pre__ngram_range':[(1,1),(1,2)],\n",
    "              'model__n_estimators':[10 , 20, 40],\n",
    "              'model__max_features': ['auto', 'sqrt', 'log2']}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        RandomForestClassifier(),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = TfidfVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC - BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'svc_bow.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "              'model__C':[0.01, 0.1, 1],\n",
    "              'model__kernel':['linear', 'poly', 'rbf'],\n",
    "              'model__gamma':['auto', 'scale']}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        SVC(),\n",
    "                        parameter,\n",
    "                        3,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = CountVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC - Tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "%%time\n",
    "\n",
    "filename = 'svc_tfidf.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "              'model__C':[0.01, 0.1, 1],\n",
    "              'model__kernel':['linear', 'poly', 'rbf'],\n",
    "              'model__gamma':['auto', 'scale']}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        SVC(),\n",
    "                        parameter,\n",
    "                        3,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = TfidfVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - Bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'lr_bow.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "             'model__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = CountVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - Tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'lr_tfidf.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "             'model__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = TfidfVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD - Bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'sgd_bow.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "             'model__alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        SGDClassifier(max_iter = 1000, loss = 'log', penalty = 'l2'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = CountVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD - Tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'sgd_tfidf.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "             'model__alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        SGDClassifier(max_iter = 1000, loss = 'log', penalty = 'l2'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = TfidfVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nb_bow.sav</td>\n",
       "      <td>0.443089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nb_tfidf.sav</td>\n",
       "      <td>0.430053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_bow.sav</td>\n",
       "      <td>0.348805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf_tfidf.sav</td>\n",
       "      <td>0.362117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>svc_bow.sav</td>\n",
       "      <td>0.430939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>svc_tfidf.sav</td>\n",
       "      <td>0.342595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lr_bow.sav</td>\n",
       "      <td>0.488090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lr_tfidf.sav</td>\n",
       "      <td>0.500102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sgd_bow.sav</td>\n",
       "      <td>0.418400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sgd_tfidf.sav</td>\n",
       "      <td>0.365344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  f1_macro\n",
       "0     nb_bow.sav  0.443089\n",
       "1   nb_tfidf.sav  0.430053\n",
       "2     rf_bow.sav  0.348805\n",
       "3   rf_tfidf.sav  0.362117\n",
       "4    svc_bow.sav  0.430939\n",
       "5  svc_tfidf.sav  0.342595\n",
       "6     lr_bow.sav  0.488090\n",
       "7   lr_tfidf.sav  0.500102\n",
       "8    sgd_bow.sav  0.418400\n",
       "9  sgd_tfidf.sav  0.365344"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = ['nb_bow.sav', 'nb_tfidf.sav', \n",
    "          'rf_bow.sav', 'rf_tfidf.sav', \n",
    "          'svc_bow.sav', 'svc_tfidf.sav', \n",
    "          'lr_bow.sav', 'lr_tfidf.sav',\n",
    "          'sgd_bow.sav', 'sgd_tfidf.sav']\n",
    "\n",
    "results = get_model_results(models, model_dir, Xtest, ytest)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Meta and BoW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the sparse features\n",
    "\n",
    "vec = TfidfVectorizer(min_df = 15, ngram_range=(1, 2))\n",
    "#vec_fit = vec.fit(train_processed.text)\n",
    "sparse_feat = vec.fit_transform(train_processed.text)\n",
    "sparse_feat\n",
    "\n",
    "# Scaling meta features\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "meta_feat_scaled = scaler.fit_transform(meta_feat.drop(\"sentiment\", axis = 1))\n",
    "\n",
    "# Combine the features\n",
    "\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "meta_feat_scaled = coo_matrix(meta_feat_scaled)\n",
    "meta_feat_scaled\n",
    "\n",
    "# Derive X and Y\n",
    "\n",
    "X = hstack([sparse_feat, meta_feat_scaled.astype(float)])\n",
    "y = train_processed.sentiment\n",
    "indices = train_processed.index\n",
    "\n",
    "# Derive train/test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename = 'lr_stacked.sav'\n",
    "\n",
    "\n",
    "parameter = {'model__C':[0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename = 'nb_stacked.sav'\n",
    "\n",
    "parameter = {'model__alpha':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        MultinomialNB(),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename = 'svc_stacked.sav'\n",
    "\n",
    "parameter = {'model__C':[0.01, 0.1, 1],\n",
    "             'model__kernel':['linear', 'poly', 'rbf'],\n",
    "             'model__gamma':['auto', 'scale']}\n",
    "\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        SVC(),\n",
    "                        parameter,\n",
    "                        3,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename = 'sgd_stacked.sav'\n",
    "\n",
    "parameter = {'model__alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2]}\n",
    "\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        SGDClassifier(),\n",
    "                        parameter,\n",
    "                        3,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nb_stacked.sav</td>\n",
       "      <td>0.430446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lr_stacked.sav</td>\n",
       "      <td>0.492030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svc_stacked.sav</td>\n",
       "      <td>0.361986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sgd_stacked.sav</td>\n",
       "      <td>0.417685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model  f1_macro\n",
       "0   nb_stacked.sav  0.430446\n",
       "1   lr_stacked.sav  0.492030\n",
       "2  svc_stacked.sav  0.361986\n",
       "3  sgd_stacked.sav  0.417685"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = ['nb_stacked.sav', 'lr_stacked.sav', 'svc_stacked.sav', 'sgd_stacked.sav']\n",
    "results = get_model_results(models, model_dir, Xtest, ytest)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec custom trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_vector(word2vec_model, n_dim, doc):\n",
    "    ### From a document, derive the mean vector of its vocabularies with embeddings from a specified word2vec model with dimension n\n",
    "    ###\n",
    "    \n",
    "    # remove out-of-vocabulary words\n",
    "    word_list = [word for word in doc if word in word2vec_model.wv.vocab]\n",
    "    \n",
    "    # Take average if the doc contains in-vocabulary words, else, take a matrix of zeroes\n",
    "    if len(word_list) >= 1:\n",
    "        return np.mean(word2vec_model[word_list], axis=0)\n",
    "    else:\n",
    "        return ([0] * n_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  25 | elapsed:    6.4s remaining:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  25 | elapsed:    8.1s remaining:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:   11.5s remaining:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:   12.1s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   12.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   12.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word embedding dimension number is 400\n",
      "0.4488771034664754\n",
      "[[ 46  32  36]\n",
      " [ 22  96  49]\n",
      " [151 188 436]]\n",
      "{'model__C': 1}\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  25 | elapsed:    6.7s remaining:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  25 | elapsed:    8.7s remaining:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:   11.9s remaining:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:   12.8s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   13.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   13.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word embedding dimension number is 400\n",
      "0.4595443279928815\n",
      "[[ 46  31  37]\n",
      " [ 11 108  48]\n",
      " [155 193 427]]\n",
      "{'model__C': 0.1}\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  25 | elapsed:    7.0s remaining:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  25 | elapsed:    9.3s remaining:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:   12.2s remaining:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:   13.3s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   13.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   13.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word embedding dimension number is 400\n",
      "0.45522944415519834\n",
      "[[ 45  32  37]\n",
      " [  9 109  49]\n",
      " [165 189 421]]\n",
      "{'model__C': 0.1}\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  25 | elapsed:    7.2s remaining:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  25 | elapsed:    9.8s remaining:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:   12.2s remaining:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:   13.6s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   13.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   13.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word embedding dimension number is 400\n",
      "0.4539968297969444\n",
      "[[ 46  35  33]\n",
      " [ 13 106  48]\n",
      " [166 186 423]]\n",
      "{'model__C': 0.1}\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  25 | elapsed:    7.6s remaining:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  25 | elapsed:   10.0s remaining:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:   12.6s remaining:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:   13.8s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   14.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   14.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word embedding dimension number is 400\n",
      "0.46020170173545544\n",
      "[[ 48  30  36]\n",
      " [ 12 108  47]\n",
      " [163 190 422]]\n",
      "{'model__C': 0.1}\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  25 | elapsed:    7.5s remaining:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  25 | elapsed:   10.5s remaining:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:   12.8s remaining:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:   14.1s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   14.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   14.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word embedding dimension number is 400\n",
      "0.45773190528803953\n",
      "[[ 49  32  33]\n",
      " [ 16 104  47]\n",
      " [157 194 424]]\n",
      "{'model__C': 0.1}\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  25 | elapsed:    9.2s remaining:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  25 | elapsed:   12.1s remaining:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:   13.7s remaining:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:   15.1s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   15.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   15.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word embedding dimension number is 400\n",
      "0.46944708646400574\n",
      "[[ 52  29  33]\n",
      " [ 15  98  54]\n",
      " [157 173 445]]\n",
      "{'model__C': 0.1}\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  25 | elapsed:    7.5s remaining:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  25 | elapsed:   10.4s remaining:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:   12.6s remaining:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:   14.0s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   14.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   14.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word embedding dimension number is 400\n",
      "0.4596076160851294\n",
      "[[ 50  29  35]\n",
      " [ 19 100  48]\n",
      " [157 186 432]]\n",
      "{'model__C': 0.1}\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  25 | elapsed:    7.5s remaining:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  25 | elapsed:   10.7s remaining:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:   12.4s remaining:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:   13.9s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   14.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   14.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word embedding dimension number is 400\n",
      "0.4575154608043878\n",
      "[[ 48  30  36]\n",
      " [ 16 101  50]\n",
      " [166 177 432]]\n",
      "{'model__C': 0.1}\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  25 | elapsed:    7.5s remaining:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  25 | elapsed:   10.6s remaining:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  25 | elapsed:   12.7s remaining:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:   14.0s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   14.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   14.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word embedding dimension number is 400\n",
      "0.46798292021071797\n",
      "[[ 52  28  34]\n",
      " [ 18 102  47]\n",
      " [154 186 435]]\n",
      "{'model__C': 0.1}\n",
      "Wall time: 12min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Create Word2vec word embeddings\n",
    "n_dim = 400\n",
    "w2v_window_grid = {}\n",
    "for n_window in np.arange(1,20,2):\n",
    "\n",
    "    sentences = [row.split() for row in train_processed.text]\n",
    "\n",
    "    cores = multiprocessing.cpu_count()\n",
    "\n",
    "    w2v_model = Word2Vec(min_count=10,\n",
    "                         window=n_window,\n",
    "                         size=n_dim,\n",
    "                         sample=6e-5, \n",
    "                         alpha=0.03, \n",
    "                         min_alpha=0.0007, \n",
    "                         negative=20,\n",
    "                         workers=cores-1)\n",
    "\n",
    "    w2v_model.build_vocab(sentences)\n",
    "\n",
    "    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30)\n",
    "\n",
    "    #### Derive word2vec features\n",
    "\n",
    "    # Create a dictionary with the vocabs and its embeddings\n",
    "    w2v = dict(zip(w2v_model.wv.index2word, w2v_model.wv.syn0))\n",
    "\n",
    "    # Initialize a vector features dictionary\n",
    "    vec_features = {}\n",
    "\n",
    "    # Iterate over the documents to get the mean of word embeddings feature to derive document embeddings\n",
    "    for index, doc in train_processed.text.iteritems(): \n",
    "        vec_features[index] = get_mean_vector(w2v_model, n_dim, doc.split(' '))\n",
    "\n",
    "    # Create a document feature dataframe\n",
    "    vec_features_df = pd.DataFrame.from_dict(vec_features, 'index')\n",
    "    vec_features_df.shape\n",
    "\n",
    "    #### Evaluate classifier with given word2vec embeddings\n",
    "\n",
    "    # Define X and y\n",
    "\n",
    "    X = vec_features_df\n",
    "    y = train_processed.sentiment\n",
    "    indices = train_processed.index\n",
    "\n",
    "    # Split train and test set\n",
    "\n",
    "    Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)\n",
    "\n",
    "    # Specify save file for model\n",
    "    filename = 'lr_w2v.sav'\n",
    "\n",
    "    # Specify parameter grid\n",
    "    parameter = {'model__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "    # Run grid search to create best classifier of type\n",
    "    clf = create_classifier(Xtrain, \n",
    "                            ytrain,\n",
    "                            LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                            parameter,\n",
    "                            5,\n",
    "                            'f1_macro',\n",
    "                            open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "    # Extract classifier score\n",
    "    f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                           Xtest,\n",
    "                                           ytest)\n",
    "\n",
    "    # Save score\n",
    "    w2v_window_grid[n_window] = f1macro_score\n",
    "\n",
    "    # Print score\n",
    "    print(\"The word embedding dimension number is\", n_dim)\n",
    "    print(f1macro_score)\n",
    "    print(cm)\n",
    "    print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_window_df = pd.DataFrame.from_dict(w2v_window_grid, orient = 'index' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_window_df.to_json(os.path.join(data_dir, 'interim', 'w2v_window_search_results.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_window_df = pd.read_json(os.path.join(data_dir, 'interim', 'w2v_window_search_results.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w2v_window_df.sort_index().plot()\n",
    "plt.title('Word2Vec window Search')\n",
    "plt.xlabel('Number of windows')\n",
    "plt.ylabel('F1 Macro Score')\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Google Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "\n",
    "w2v_google_model = KeyedVectors.load_word2vec_format(os.path.join(model_dir, 'embeddings', 'GoogleNews-vectors-negative300.bin'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the vocabs and its embeddings\n",
    "w2v = dict(zip(w2v_google_model.wv.index2word, w2v_google_model.wv.syn0))\n",
    "\n",
    "# Initialize a vector features dictionary\n",
    "vec_features = {}\n",
    "\n",
    "# Iterate over the documents to get the mean of word embeddings feature to derive document embeddings\n",
    "for index, doc in train_processed.text.iteritems(): \n",
    "    vec_features[index] = get_mean_vector(w2v_google_model, 300, doc.split(' '))\n",
    "\n",
    "# Create a document feature dataframe\n",
    "vec_features_df = pd.DataFrame.from_dict(vec_features, 'index')\n",
    "vec_features_df.shape\n",
    "\n",
    "#### Evaluate classifier with given word2vec embeddings\n",
    "\n",
    "# Define X and y\n",
    "\n",
    "X = vec_features_df\n",
    "y = train_processed.sentiment\n",
    "indices = train_processed.index\n",
    "\n",
    "# Split train and test set\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)\n",
    "\n",
    "# Specify save file for model\n",
    "filename = 'lr_w2v_pretrained_google.sav'\n",
    "\n",
    "# Specify parameter grid\n",
    "parameter = {'model__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Run grid search to create best classifier of type\n",
    "clf = create_classifier(Xtrain, \n",
    "                        ytrain,\n",
    "                        LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Stanford Pre-trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_vectors_web_lg')\n",
    "\n",
    "total_vectors = len(nlp.vocab.vectors)\n",
    "print('Total word vectors:', total_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = os.path.join(model_dir, 'embeddings', 'glove.6B.300d.txt')\n",
    "word2vec_output_file = os.path.join(model_dir, 'embeddings','glove.6B.300d.txt.word2vec')\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "word2vec_output_file = os.path.join(model_dir, 'embeddings','glove.6B.300d.txt.word2vec')\n",
    "w2v_glove_stanford_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the vocabs and its embeddings\n",
    "w2v = dict(zip(w2v_glove_stanford_model.wv.index2word, w2v_glove_stanford_model.wv.syn0))\n",
    "\n",
    "# Initialize a vector features dictionary\n",
    "vec_features = {}\n",
    "\n",
    "# Iterate over the documents to get the mean of word embeddings feature to derive document embeddings\n",
    "for index, doc in train_processed.text.iteritems(): \n",
    "    vec_features[index] = get_mean_vector(w2v_glove_stanford_model, 300, doc.split(' '))\n",
    "\n",
    "# Create a document feature dataframe\n",
    "vec_features_df = pd.DataFrame.from_dict(vec_features, 'index')\n",
    "vec_features_df.shape\n",
    "\n",
    "#### Evaluate classifier with given word2vec embeddings\n",
    "\n",
    "# Define X and y\n",
    "\n",
    "X = vec_features_df\n",
    "y = train_processed.sentiment\n",
    "indices = train_processed.index\n",
    "\n",
    "# Split train and test set\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)\n",
    "\n",
    "# Specify save file for model\n",
    "filename = 'lr_w2v_pretrained_glove_stanford.sav'\n",
    "\n",
    "# Specify parameter grid\n",
    "parameter = {'model__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Run grid search to create best classifier of type\n",
    "clf = create_classifier(Xtrain, \n",
    "                        ytrain,\n",
    "                        LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(doc) for document in train_processed.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "feature_size = 300    # Word vector dimensionality  \n",
    "window_context = 50          # Context window size                                                                                    \n",
    "min_word_count = 5   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "ft_model = FastText(tokenized_corpus, \n",
    "                    size=feature_size, \n",
    "                    window=window_context, \n",
    "                    min_count=min_word_count,\n",
    "                    sample=sample, \n",
    "                    sg=1, \n",
    "                    iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the vocabs and its embeddings\n",
    "w2v = dict(zip(ft_model.wv.index2word, ft_model.wv.syn0))\n",
    "\n",
    "# Initialize a vector features dictionary\n",
    "vec_features = {}\n",
    "\n",
    "# Iterate over the documents to get the mean of word embeddings feature to derive document embeddings\n",
    "for index, doc in train_processed.text.iteritems(): \n",
    "    vec_features[index] = get_mean_vector(ft_model, 300, doc.split(' '))\n",
    "\n",
    "# Create a document feature dataframe\n",
    "vec_features_df = pd.DataFrame.from_dict(vec_features, 'index')\n",
    "vec_features_df.shape\n",
    "\n",
    "#### Evaluate classifier with given word2vec embeddings\n",
    "\n",
    "# Define X and y\n",
    "\n",
    "X = vec_features_df\n",
    "y = train_processed.sentiment\n",
    "indices = train_processed.index\n",
    "\n",
    "# Split train and test set\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)\n",
    "\n",
    "# Specify save file for model\n",
    "filename = 'lr_w2v_fasttext.sav'\n",
    "\n",
    "# Specify parameter grid\n",
    "parameter = {'model__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Run grid search to create best classifier of type\n",
    "clf = create_classifier(Xtrain, \n",
    "                        ytrain,\n",
    "                        LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights = pd.DataFrame(w2v).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['patient', 'drug', 'therapy', 'treatment', 'vaccine', 'health', 'public','effective']}\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "wvs = w2v_model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
