{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore deprecation warnings in sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data directory\n",
    "\n",
    "data_dir = os.path.join(os.path.dirname(os.getcwd()),'Data')\n",
    "\n",
    "# Set model directory\n",
    "\n",
    "model_dir = os.path.join(os.path.dirname(os.getcwd()), 'Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data paths\n",
    "\n",
    "train_path = os.path.join(data_dir, 'train.csv')\n",
    "\n",
    "train_processed_path = os.path.join(data_dir, 'interim', 'train_preprocessed.txt')\n",
    "\n",
    "meta_feat_path = os.path.join(data_dir, 'interim', 'meta_feat.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path)\n",
    "train_processed = pd.read_json(train_processed_path)\n",
    "meta_feat = pd.read_json(meta_feat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>n_upper</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_wlen</th>\n",
       "      <th>adj_drug_count</th>\n",
       "      <th>n_stop</th>\n",
       "      <th>n_num</th>\n",
       "      <th>drug_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>404</td>\n",
       "      <td>5.315789</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>206</td>\n",
       "      <td>1184</td>\n",
       "      <td>5.747573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>135</td>\n",
       "      <td>780</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>124</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>115</td>\n",
       "      <td>612</td>\n",
       "      <td>5.321739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  n_upper  word_count  char_count  avg_wlen  adj_drug_count  \\\n",
       "0          2        1          76         404  5.315789        0.026316   \n",
       "1          2        5         206        1184  5.747573        0.000000   \n",
       "2          2        4         135         780  5.777778        0.007407   \n",
       "3          2        1          20         124  6.200000        0.100000   \n",
       "4          1        8         115         612  5.321739        0.000000   \n",
       "\n",
       "   n_stop  n_num  drug_category  \n",
       "0      28      0             38  \n",
       "1      85      0             38  \n",
       "2      50      3             35  \n",
       "3       5      0             64  \n",
       "4      44      0             38  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Naive Bayes\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from custom_function.ml import create_classifier, evaluate_classifier, get_model_results\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Blue'>Baseline Model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define X and y\n",
    "\n",
    "X = meta_feat.drop(\"sentiment\", axis = 1)\n",
    "y = meta_feat.sentiment\n",
    "indices = train_processed.index\n",
    "\n",
    "# Split train and test set\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'baseline_lr.sav'\n",
    "\n",
    "parameter = {'model__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = StandardScaler())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unique, counts = np.unique(clf.predict(Xtest), return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Blue'>Bag of Word models</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "\n",
    "X = train_processed.text\n",
    "y = train_processed.sentiment\n",
    "indices = train_processed.index\n",
    "\n",
    "# Split train and test set\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Naive Bayes - BoW</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'nb_bow.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "             'model__alpha':[0.01, 0.1, 1, 10]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        MultinomialNB(),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = CountVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Naive Bayes - Tfidf</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'nb_tfidf.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "             'model__alpha':[0.01, 0.1, 1, 10]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        MultinomialNB(),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = TfidfVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='green'>Random Forest - BoW</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'rf_bow.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "              'pre__ngram_range':[(1,1),(1,2)],\n",
    "              'model__n_estimators':[10 , 20, 40],\n",
    "              'model__max_features': ['auto', 'sqrt', 'log2']}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        RandomForestClassifier(),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = CountVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='green'>Random Forest - TfIdf</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'rf_tfidf.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "              'pre__ngram_range':[(1,1),(1,2)],\n",
    "              'model__n_estimators':[10 , 20, 40],\n",
    "              'model__max_features': ['auto', 'sqrt', 'log2']}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        RandomForestClassifier(),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = TfidfVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='green'>SVC - BoW</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'svc_bow.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "              'model__C':[0.01, 0.1, 1],\n",
    "              'model__kernel':['linear', 'poly', 'rbf'],\n",
    "              'model__gamma':['auto', 'scale']}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        SVC(),\n",
    "                        parameter,\n",
    "                        3,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = CountVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='green'>SVC - TfIdf</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "%%time\n",
    "\n",
    "filename = 'svc_tfidf.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "              'model__C':[0.01, 0.1, 1],\n",
    "              'model__kernel':['linear', 'poly', 'rbf'],\n",
    "              'model__gamma':['auto', 'scale']}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        SVC(),\n",
    "                        parameter,\n",
    "                        3,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = TfidfVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='green'>Logistic Regression - BoW</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   26.8s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   54.5s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 12.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48998528133196806\n",
      "[[ 37  19  55]\n",
      " [ 15  75  85]\n",
      " [ 89 105 576]]\n",
      "{'model__C': 0.01, 'pre__min_df': 1, 'pre__ngram_range': (1, 2)}\n",
      "Wall time: 12min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "filename = 'lr_bow.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "             'model__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = CountVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='green'>Logistic Regression - TfIdf</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'lr_tfidf.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "             'model__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = TfidfVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='green'>SGD - BoW</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'sgd_bow.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "             'model__alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        SGDClassifier(max_iter = 1000, loss = 'log', penalty = 'l2'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = CountVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='green'>SGD - TfIdf</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "filename = 'sgd_tfidf.sav'\n",
    "\n",
    "parameter = {'pre__min_df':[0.01, 0.1, 1, 10],\n",
    "             'pre__ngram_range':[(1,1),(1,2)],\n",
    "             'model__alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        SGDClassifier(max_iter = 1000, loss = 'log', penalty = 'l2'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'),\n",
    "                        preprocess = TfidfVectorizer())\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interesting grand merci wonder lemtrada ocrevus sale would go prove anti-cd20 induction'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed.text[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nb_bow.sav</td>\n",
       "      <td>0.852339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nb_tfidf.sav</td>\n",
       "      <td>0.775414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_bow.sav</td>\n",
       "      <td>0.893952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf_tfidf.sav</td>\n",
       "      <td>0.900964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>svc_bow.sav</td>\n",
       "      <td>0.856940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>svc_tfidf.sav</td>\n",
       "      <td>0.487064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lr_bow.sav</td>\n",
       "      <td>0.489985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lr_tfidf.sav</td>\n",
       "      <td>0.747727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sgd_bow.sav</td>\n",
       "      <td>0.816405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sgd_tfidf.sav</td>\n",
       "      <td>0.550691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  f1_macro\n",
       "0     nb_bow.sav  0.852339\n",
       "1   nb_tfidf.sav  0.775414\n",
       "2     rf_bow.sav  0.893952\n",
       "3   rf_tfidf.sav  0.900964\n",
       "4    svc_bow.sav  0.856940\n",
       "5  svc_tfidf.sav  0.487064\n",
       "6     lr_bow.sav  0.489985\n",
       "7   lr_tfidf.sav  0.747727\n",
       "8    sgd_bow.sav  0.816405\n",
       "9  sgd_tfidf.sav  0.550691"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = ['nb_bow.sav', 'nb_tfidf.sav', \n",
    "          'rf_bow.sav', 'rf_tfidf.sav', \n",
    "          'svc_bow.sav', 'svc_tfidf.sav', \n",
    "          'lr_bow.sav', 'lr_tfidf.sav',\n",
    "          'sgd_bow.sav', 'sgd_tfidf.sav']\n",
    "\n",
    "results = get_model_results(models, model_dir, Xtest, ytest)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Example:</b> Logistic Regression trained on previous dataset did a much better job than the one trained on this dataset\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pickle.load(open(os.path.join(model_dir, 'lr_tfidf.sav'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('pre', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...enalty='l2', random_state=None,\n",
       "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'pre__min_df': [0.01, 0.1, 1, 10], 'pre__ngram_range': [(1, 1), (1, 2)], 'model__C': [0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring='f1_macro', verbose=10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 94   6  11]\n",
      " [  5 149  21]\n",
      " [ 77  82 611]]\n",
      "0.7477266152438963\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest,clf.predict(Xtest)))\n",
    "print(f1_score(ytest, \n",
    "               clf.predict(Xtest), average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('pre', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...enalty='l2', random_state=None,\n",
       "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'pre__min_df': [0.01, 0.1, 1, 10], 'pre__ngram_range': [(1, 1), (1, 2)], 'model__C': [0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring='f1_macro', verbose=10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(ytest,clf.predict(Xtest)))\n",
    "print(f1_score(ytest, \n",
    "               clf.predict(Xtest), average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Meta and BoW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the sparse features\n",
    "\n",
    "vec = TfidfVectorizer(min_df = 15, ngram_range=(1, 2))\n",
    "#vec_fit = vec.fit(train_processed.text)\n",
    "sparse_feat = vec.fit_transform(train_processed.text)\n",
    "sparse_feat\n",
    "\n",
    "# Scaling meta features\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "meta_feat_scaled = scaler.fit_transform(meta_feat.drop(\"sentiment\", axis = 1))\n",
    "\n",
    "# Combine the features\n",
    "\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "meta_feat_scaled = coo_matrix(meta_feat_scaled)\n",
    "meta_feat_scaled\n",
    "\n",
    "# Derive X and Y\n",
    "\n",
    "X = hstack([sparse_feat, meta_feat_scaled.astype(float)])\n",
    "y = train_processed.sentiment\n",
    "indices = train_processed.index\n",
    "\n",
    "# Derive train/test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename = 'lr_stacked.sav'\n",
    "\n",
    "\n",
    "parameter = {'model__C':[0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename = 'nb_stacked.sav'\n",
    "\n",
    "parameter = {'model__alpha':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        MultinomialNB(),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename = 'svc_stacked.sav'\n",
    "\n",
    "parameter = {'model__C':[0.01, 0.1, 1],\n",
    "             'model__kernel':['linear', 'poly', 'rbf'],\n",
    "             'model__gamma':['auto', 'scale']}\n",
    "\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        SVC(),\n",
    "                        parameter,\n",
    "                        3,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename = 'sgd_stacked.sav'\n",
    "\n",
    "parameter = {'model__alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2]}\n",
    "\n",
    "\n",
    "clf = create_classifier(Xtrain,\n",
    "                        ytrain,\n",
    "                        SGDClassifier(),\n",
    "                        parameter,\n",
    "                        3,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = ['nb_stacked.sav', 'lr_stacked.sav', 'svc_stacked.sav', 'sgd_stacked.sav']\n",
    "results = get_model_results(models, model_dir, Xtest, ytest)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec custom trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_vector(word2vec_model, n_dim, doc):\n",
    "    \"\"\" \n",
    "    From a document, derive the mean vector of its vocabularies with embeddings from a specified word2vec model with dimension n\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove out-of-vocabulary words\n",
    "    word_list = [word for word in doc if word in word2vec_model.wv.vocab]\n",
    "    \n",
    "    # Take average if the doc contains in-vocabulary words, else, take a matrix of zeroes\n",
    "    if len(word_list) >= 1:\n",
    "        return np.mean(word2vec_model[word_list], axis=0)\n",
    "    else:\n",
    "        return ([0] * n_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### Create Word2vec word embeddings\n",
    "n_dim = 400\n",
    "w2v_window_grid = {}\n",
    "for n_window in np.arange(1,20,2):\n",
    "\n",
    "    sentences = [row.split() for row in train_processed.text]\n",
    "\n",
    "    cores = multiprocessing.cpu_count()\n",
    "\n",
    "    w2v_model = Word2Vec(min_count=10,\n",
    "                         window=n_window,\n",
    "                         size=n_dim,\n",
    "                         sample=6e-5, \n",
    "                         alpha=0.03, \n",
    "                         min_alpha=0.0007, \n",
    "                         negative=20,\n",
    "                         workers=cores-1)\n",
    "\n",
    "    w2v_model.build_vocab(sentences)\n",
    "\n",
    "    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30)\n",
    "\n",
    "    #### Derive word2vec features\n",
    "\n",
    "    # Create a dictionary with the vocabs and its embeddings\n",
    "    w2v = dict(zip(w2v_model.wv.index2word, w2v_model.wv.syn0))\n",
    "\n",
    "    # Initialize a vector features dictionary\n",
    "    vec_features = {}\n",
    "\n",
    "    # Iterate over the documents to get the mean of word embeddings feature to derive document embeddings\n",
    "    for index, doc in train_processed.text.iteritems(): \n",
    "        vec_features[index] = get_mean_vector(w2v_model, n_dim, doc.split(' '))\n",
    "\n",
    "    # Create a document feature dataframe\n",
    "    vec_features_df = pd.DataFrame.from_dict(vec_features, 'index')\n",
    "    vec_features_df.shape\n",
    "\n",
    "    #### Evaluate classifier with given word2vec embeddings\n",
    "\n",
    "    # Define X and y\n",
    "\n",
    "    X = vec_features_df\n",
    "    y = train_processed.sentiment\n",
    "    indices = train_processed.index\n",
    "\n",
    "    # Split train and test set\n",
    "\n",
    "    Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)\n",
    "\n",
    "    # Specify save file for model\n",
    "    filename = 'lr_w2v.sav'\n",
    "\n",
    "    # Specify parameter grid\n",
    "    parameter = {'model__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "    # Run grid search to create best classifier of type\n",
    "    clf = create_classifier(Xtrain, \n",
    "                            ytrain,\n",
    "                            LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                            parameter,\n",
    "                            5,\n",
    "                            'f1_macro',\n",
    "                            open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "    # Extract classifier score\n",
    "    f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                           Xtest,\n",
    "                                           ytest)\n",
    "\n",
    "    # Save score\n",
    "    w2v_window_grid[n_window] = f1macro_score\n",
    "\n",
    "    # Print score\n",
    "    print(\"The word embedding dimension number is\", n_dim)\n",
    "    print(f1macro_score)\n",
    "    print(cm)\n",
    "    print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_window_df = pd.DataFrame.from_dict(w2v_window_grid, orient = 'index' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_window_df.to_json(os.path.join(data_dir, 'interim', 'w2v_window_search_results.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_window_df = pd.read_json(os.path.join(data_dir, 'interim', 'w2v_window_search_results.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_window_df.sort_index().plot()\n",
    "plt.title('Word2Vec window Search')\n",
    "plt.xlabel('Number of windows')\n",
    "plt.ylabel('F1 Macro Score')\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Google Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "\n",
    "w2v_google_model = KeyedVectors.load_word2vec_format(os.path.join(model_dir, 'embeddings', 'GoogleNews-vectors-negative300.bin'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the vocabs and its embeddings\n",
    "w2v = dict(zip(w2v_google_model.wv.index2word, w2v_google_model.wv.syn0))\n",
    "\n",
    "# Initialize a vector features dictionary\n",
    "vec_features = {}\n",
    "\n",
    "# Iterate over the documents to get the mean of word embeddings feature to derive document embeddings\n",
    "for index, doc in train_processed.text.iteritems(): \n",
    "    vec_features[index] = get_mean_vector(w2v_google_model, 300, doc.split(' '))\n",
    "\n",
    "# Create a document feature dataframe\n",
    "vec_features_df = pd.DataFrame.from_dict(vec_features, 'index')\n",
    "vec_features_df.shape\n",
    "\n",
    "#### Evaluate classifier with given word2vec embeddings\n",
    "\n",
    "# Define X and y\n",
    "\n",
    "X = vec_features_df\n",
    "y = train_processed.sentiment\n",
    "indices = train_processed.index\n",
    "\n",
    "# Split train and test set\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)\n",
    "\n",
    "# Specify save file for model\n",
    "filename = 'lr_w2v_pretrained_google.sav'\n",
    "\n",
    "# Specify parameter grid\n",
    "parameter = {'model__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Run grid search to create best classifier of type\n",
    "clf = create_classifier(Xtrain, \n",
    "                        ytrain,\n",
    "                        LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Stanford Pre-trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_vectors_web_lg')\n",
    "\n",
    "total_vectors = len(nlp.vocab.vectors)\n",
    "print('Total word vectors:', total_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = os.path.join(model_dir, 'embeddings', 'glove.6B.300d.txt')\n",
    "word2vec_output_file = os.path.join(model_dir, 'embeddings','glove.6B.300d.txt.word2vec')\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "word2vec_output_file = os.path.join(model_dir, 'embeddings','glove.6B.300d.txt.word2vec')\n",
    "w2v_glove_stanford_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the vocabs and its embeddings\n",
    "w2v = dict(zip(w2v_glove_stanford_model.wv.index2word, w2v_glove_stanford_model.wv.syn0))\n",
    "\n",
    "# Initialize a vector features dictionary\n",
    "vec_features = {}\n",
    "\n",
    "# Iterate over the documents to get the mean of word embeddings feature to derive document embeddings\n",
    "for index, doc in train_processed.text.iteritems(): \n",
    "    vec_features[index] = get_mean_vector(w2v_glove_stanford_model, 300, doc.split(' '))\n",
    "\n",
    "# Create a document feature dataframe\n",
    "vec_features_df = pd.DataFrame.from_dict(vec_features, 'index')\n",
    "vec_features_df.shape\n",
    "\n",
    "#### Evaluate classifier with given word2vec embeddings\n",
    "\n",
    "# Define X and y\n",
    "\n",
    "X = vec_features_df\n",
    "y = train_processed.sentiment\n",
    "indices = train_processed.index\n",
    "\n",
    "# Split train and test set\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)\n",
    "\n",
    "# Specify save file for model\n",
    "filename = 'lr_w2v_pretrained_glove_stanford.sav'\n",
    "\n",
    "# Specify parameter grid\n",
    "parameter = {'model__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Run grid search to create best classifier of type\n",
    "clf = create_classifier(Xtrain, \n",
    "                        ytrain,\n",
    "                        LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(doc) for document in train_processed.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "feature_size = 300    # Word vector dimensionality  \n",
    "window_context = 50          # Context window size                                                                                    \n",
    "min_word_count = 5   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "ft_model = FastText(tokenized_corpus, \n",
    "                    size=feature_size, \n",
    "                    window=window_context, \n",
    "                    min_count=min_word_count,\n",
    "                    sample=sample, \n",
    "                    sg=1, \n",
    "                    iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the vocabs and its embeddings\n",
    "w2v = dict(zip(ft_model.wv.index2word, ft_model.wv.syn0))\n",
    "\n",
    "# Initialize a vector features dictionary\n",
    "vec_features = {}\n",
    "\n",
    "# Iterate over the documents to get the mean of word embeddings feature to derive document embeddings\n",
    "for index, doc in train_processed.text.iteritems(): \n",
    "    vec_features[index] = get_mean_vector(ft_model, 300, doc.split(' '))\n",
    "\n",
    "# Create a document feature dataframe\n",
    "vec_features_df = pd.DataFrame.from_dict(vec_features, 'index')\n",
    "vec_features_df.shape\n",
    "\n",
    "#### Evaluate classifier with given word2vec embeddings\n",
    "\n",
    "# Define X and y\n",
    "\n",
    "X = vec_features_df\n",
    "y = train_processed.sentiment\n",
    "indices = train_processed.index\n",
    "\n",
    "# Split train and test set\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)\n",
    "\n",
    "# Specify save file for model\n",
    "filename = 'lr_w2v_fasttext.sav'\n",
    "\n",
    "# Specify parameter grid\n",
    "parameter = {'model__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Run grid search to create best classifier of type\n",
    "clf = create_classifier(Xtrain, \n",
    "                        ytrain,\n",
    "                        LogisticRegression(solver = 'lbfgs', class_weight = 'balanced'),\n",
    "                        parameter,\n",
    "                        5,\n",
    "                        'f1_macro',\n",
    "                        open(os.path.join(model_dir,filename), 'wb'))\n",
    "\n",
    "# Extract classifier score\n",
    "f1macro_score, cm = evaluate_classifier(clf,\n",
    "                                       Xtest,\n",
    "                                       ytest)\n",
    "\n",
    "# Print score\n",
    "print(f1macro_score)\n",
    "print(cm)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights = pd.DataFrame(w2v).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['patient', 'drug', 'therapy', 'treatment', 'vaccine', 'health', 'public','effective']}\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "wvs = w2v_model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
