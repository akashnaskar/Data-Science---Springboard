{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Import, Directory and Preference settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.options.display.max_columns=100\n",
    "pd.options.display.max_rows=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore deprecation warnings in sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data directory\n",
    "\n",
    "data_dir = os.path.join(os.path.dirname(os.getcwd()),'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data paths\n",
    "\n",
    "data_path = os.path.join(data_dir, 'Data')\n",
    "\n",
    "train_path = os.path.join(data_dir, 'train.csv')\n",
    "\n",
    "holdout_path = os.path.join(data_dir, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model path\n",
    "\n",
    "model_dir = os.path.join(os.path.dirname(os.getcwd()), 'Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "holdout = pd.read_csv(holdout_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5279, 4)\n",
      "(2924, 3)\n",
      "0.6435450444959161\n"
     ]
    }
   ],
   "source": [
    "# Print the dimensions of both sets and the split percentage of training set\n",
    "\n",
    "print(train.shape)\n",
    "print(holdout.shape)\n",
    "print(len(train)/(len(holdout) + len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5279 entries, 0 to 5278\n",
      "Data columns (total 4 columns):\n",
      "unique_hash    5279 non-null object\n",
      "text           5279 non-null object\n",
      "drug           5279 non-null object\n",
      "sentiment      5279 non-null int64\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 165.0+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2924 entries, 0 to 2923\n",
      "Data columns (total 3 columns):\n",
      "unique_hash    2924 non-null object\n",
      "text           2924 non-null object\n",
      "drug           2924 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 68.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print the info columns of both sets\n",
    "\n",
    "print(train.info())\n",
    "print(holdout.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                unique_hash  \\\n",
      "0  2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0   \n",
      "1  9eba8f80e7e20f3a2f48685530748fbfa95943e4   \n",
      "2  fe809672251f6bd0d986e00380f48d047c7e7b76   \n",
      "3  bd22104dfa9ec80db4099523e03fae7a52735eb6   \n",
      "4  b227688381f9b25e5b65109dd00f7f895e838249   \n",
      "\n",
      "                                                text        drug  sentiment  \n",
      "0  Autoimmune diseases tend to come in clusters. ...     gilenya          2  \n",
      "1  I can completely understand why you’d want to ...     gilenya          2  \n",
      "2  Interesting that it only targets S1P-1/5 recep...  fingolimod          2  \n",
      "3  Very interesting, grand merci. Now I wonder wh...     ocrevus          2  \n",
      "4  Hi everybody, My latest MRI results for Brain ...     gilenya          1  \n"
     ]
    }
   ],
   "source": [
    "# View 5 first rows in the train set\n",
    "\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_hash    0\n",
      "text           0\n",
      "drug           0\n",
      "sentiment      0\n",
      "dtype: int64\n",
      "unique_hash    0\n",
      "text           0\n",
      "drug           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Consider the missingness of the data\n",
    "\n",
    "print(np.sum(train.isna()))\n",
    "print(np.sum(holdout.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_hash    5279\n",
      "text           5181\n",
      "drug            102\n",
      "sentiment         3\n",
      "dtype: int64\n",
      "unique_hash    2924\n",
      "text           2721\n",
      "drug             95\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Consider the classes of the values in each set\n",
    "\n",
    "print(train.nunique())\n",
    "print(holdout.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Preprocessing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-success\">\n",
    "We are most interested in the sentiment classes (our label) and the drug classes. It seems there are 102 drugs being reviewed about and 3 sentiments placed on them (positive, negative or neutral)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A great post Sue. You have wrapped it right up. Incidentally, the new Ocrevus isn't for people who have a history of cancer, I was informed by my neurologist. Thus snatching away any hope of getting it prescribed even before NICE did. I think this forum is a great little community. I would really miss it if it wasn't there! It's good to be able to pop in and out as and when the mood takes me. Apart from the really useful information available, I like the rants, the recipies, the laughter, the ramblings. I love being able to talk away to myself on here, when I need to unload some of the pressure in my head. What's more, no one can see my tears on here...\n"
     ]
    }
   ],
   "source": [
    "# Next, we must clean the data. First, let's take a random review\n",
    "\n",
    "sample_text = train.text[random.randint(1,len(train))]\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Zach\n",
      "[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Zach\n",
      "[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Zach Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\Zach Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Define cleaning modules and cleaning functions\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from custom_function.contractions import CONTRACTION_MAP\n",
    "from unicodedata import normalize\n",
    "\n",
    "# Import nltk resources\n",
    "resources = [\"wordnet\", \"stopwords\", \"punkt\", \\\n",
    "             \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"]\n",
    "\n",
    "for resource in resources:\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/\" + resource)\n",
    "    except LookupError:\n",
    "        nltk.download(resource)\n",
    "\n",
    "# Create stopwords list        \n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Define lemmatizing functions\n",
    "def lemmatize_doc(document):\n",
    "    \"\"\" \n",
    "    Conduct pre-processing, tag words then returns sentence with lemmatized words\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an empty list of lemmatized tokens\n",
    "    lemmatized_list = []\n",
    "    \n",
    "    # Tokenize the sentences\n",
    "    tokenized_sent = sent_tokenize(document)\n",
    "    \n",
    "    # Iterate over sentences to conduct lemmatization\n",
    "    for sentence in tokenized_sent:\n",
    "        \n",
    "        # Tokenize the words in the sentence\n",
    "        tokenized_word = word_tokenize(sentence)\n",
    "        \n",
    "        # Tag the pos of the tokens\n",
    "        tagged_token = pos_tag(tokenized_word)\n",
    "        \n",
    "        # Initialize a empty list of lemmatized words\n",
    "        root = []\n",
    "\n",
    "        # Create Lemmatizer object\n",
    "        lemma = WordNetLemmatizer()\n",
    "\n",
    "        # iterate over the tagged sentences to \n",
    "        for token in tagged_token:\n",
    "\n",
    "            # assign tag and actual word of the token\n",
    "            tag = token[1][0]\n",
    "            word = token[0]\n",
    "\n",
    "            # Lemmatize the token based on tags\n",
    "            if tag.startswith('J'):\n",
    "                root.append(lemma.lemmatize(word, wordnet.ADJ))\n",
    "            elif tag.startswith('V'):\n",
    "                root.append(lemma.lemmatize(word, wordnet.VERB))\n",
    "            elif tag.startswith('N'):\n",
    "                root.append(lemma.lemmatize(word, wordnet.NOUN))\n",
    "            elif tag.startswith('R'):\n",
    "                root.append(lemma.lemmatize(word, wordnet.ADV))\n",
    "            else:          \n",
    "                root.append(word)\n",
    "\n",
    "        # Add the lemmatized word into our list\n",
    "        lemmatized_list.extend(root)\n",
    "        \n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \"\"\"\n",
    "    Expand the contractions form to create cohenrent extractions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Substitute quotation marks with apostrophes\n",
    "    text = re.sub(\"’\", \"'\", text)\n",
    "    \n",
    "    # define the contraction pattern with custom contraction mappings\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    # Define function to expand contraction matches\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "# Define main text cleaning function\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Return a processed version of the text given\n",
    "    \"\"\"\n",
    "    # Turn all text into lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand all contractions\n",
    "    text = expand_contractions(text)\n",
    "    \n",
    "    # Remove all links\n",
    "    text = re.sub(r'www.[^ ]+', '', text)\n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    \n",
    "    # Remove all punctuations, except hyphens\n",
    "    text = re.sub(r\"[%;$`“”\\,.!?():\\[\\]\\/]\", ' ', text)\n",
    "\n",
    "    # Remove all shortened words (like \" d \" from \"you'd\")\n",
    "    #no_shorten = re.sub(r\"\\s[dtms]\\s\", ' ', no_punctuation)\n",
    "    #no_shorten = re.sub(r\"\\sre\\s\", ' ', no_shorten)\n",
    "\n",
    "    # Remove all numerics stands by itself\n",
    "    text = re.sub(r\"(?<=\\s)\\d+(?=\\s)\", ' ', text)\n",
    "\n",
    "    # Lemmatize text\n",
    "    text = lemmatize_doc(text)\n",
    "    \n",
    "    # Remove stand-alone hyphens\n",
    "    text = re.sub(r\"\\s-\\s\", ' ', text)\n",
    "    \n",
    "    # Removing Extra spaces\n",
    "    text = re.sub(r'[\\s]+', ' ', text)\n",
    "    \n",
    "    # Convert \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great post sue wrap right up incidentally new ocrevus people history cancer inform neurologist thus snatch away hope get prescribe even nice do think forum great little community would really miss there good able pop mood take me apart really useful information available like rant recipies laughter ramblings love able talk away here need unload pressure head more one see tear here'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the sample text after cleaning\n",
    "clean_text(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Apply the pre-processing algorithm into a new dataframe: train_processed\n",
    "\n",
    "train_processed = train[['text', 'drug', 'sentiment']]\n",
    "holdout_processed = holdout[['text', 'drug']]\n",
    "\n",
    "train_processed.text = train_processed.text.apply(clean_text)\n",
    "holdout_processed.text = holdout_processed.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed.to_json(os.path.join(data_dir, 'interim', 'train_preprocessed.txt'))\n",
    "holdout_processed.to_json(os.path.join(data_dir, 'interim', 'holdout_preprocessed.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed = pd.read_json(os.path.join(data_dir, 'interim', 'train_preprocessed.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>autoimmune disease tend come cluster gilenya –...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>completely understand would want try it but re...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>vision one eye unrelated eye injection however...</td>\n",
       "      <td>lucentis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>apr alana write hello everyone nscl stag diagn...</td>\n",
       "      <td>tarceva</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>cladribine indeed approve would effective prof...</td>\n",
       "      <td>fingolimod</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text        drug  sentiment\n",
       "0     autoimmune disease tend come cluster gilenya –...     gilenya          2\n",
       "1     completely understand would want try it but re...     gilenya          2\n",
       "10    vision one eye unrelated eye injection however...    lucentis          1\n",
       "100   apr alana write hello everyone nscl stag diagn...     tarceva          1\n",
       "1000  cladribine indeed approve would effective prof...  fingolimod          2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
